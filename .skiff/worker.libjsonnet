function(
    indexId,
    env,
    envVariables,
    volumes,
    volumeMounts,
    config,
    podLabels,
    fullyQualifiedName,
    labels,
    namespaceName,
    annotations,
    cause,
    antiAffinityLabels,
    workerImage
)
    local attributionWorkerReplicas = if env == 'prod' then config.attributionWorkerReplicas.prod else 1;
    local attributionWorkerSuffix = '-attribution-worker' + '-' + indexId;
    local attributionWorkerSelectorLabels = {
        app: config.appName + attributionWorkerSuffix,
        env: env
    };
    local attributionWorkerPodLabels = podLabels + { app: config.appName + attributionWorkerSuffix };
    local attributionWorkerFullyQualifiedName = fullyQualifiedName + attributionWorkerSuffix;

    // This is used to verify that the worker is functional.
    local workerHealthCheck = {
        port: 8080,
        scheme: 'HTTP'
    };

    local attributionWorkerDeployment = {
        apiVersion: 'apps/v1',
        kind: 'Deployment',
        metadata: {
            labels: labels,
            name: attributionWorkerFullyQualifiedName,
            namespace: namespaceName,
            annotations: annotations + {
                'kubernetes.io/change-cause': cause
            }
        },
        spec: {
            strategy: {
                type: 'RollingUpdate',
                rollingUpdate: {
                    maxSurge: attributionWorkerReplicas // This makes deployments faster.
                }
            },
            revisionHistoryLimit: 3,
            replicas: attributionWorkerReplicas,
            selector: {
                matchLabels: attributionWorkerSelectorLabels
            },
            template: {
                metadata: {
                    name: attributionWorkerFullyQualifiedName,
                    namespace: namespaceName,
                    labels: attributionWorkerPodLabels,
                    annotations: annotations
                },
                spec: {
                    # This block tells the cluster that we'd like to make sure
                    # each instance of your application is on a different node. This
                    # way if a node goes down, your application doesn't:
                    # See: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-isolation-restriction
                    affinity: {
                        podAntiAffinity: {
                            requiredDuringSchedulingIgnoredDuringExecution: [
                                {
                                   labelSelector: {
                                        matchExpressions: [
                                            {
                                                    key: labelName,
                                                    operator: "In",
                                                    values: [ antiAffinityLabels[labelName], ],
                                            } for labelName in std.objectFields(antiAffinityLabels)
                                       ],
                                    },
                                    topologyKey: "kubernetes.io/hostname"
                                },
                            ]
                        },
                    },
                    nodeSelector: {
                        "cloud.google.com/gke-nodepool": "cpu64-256g"
                    },
                    volumes: volumes,
                    containers: [
                        {
                            name: attributionWorkerFullyQualifiedName,
                            image: workerImage,
                            volumeMounts: volumeMounts,
                            # The "probes" below allow Kubernetes to determine
                            # if your application is working properly.
                            #
                            # The readinessProbe is used to determine if
                            # an instance of your application can accept live
                            # requests. The configuration below tells Kubernetes
                            # to stop sending live requests to your application
                            # if it returns 3 non 2XX responses over 30 seconds.
                            # When this happens the application instance will
                            # be taken out of rotation and given time to "catch-up".
                            # Once it returns a single 2XX, Kubernetes will put
                            # it back in rotation.
                            #
                            # Kubernetes also has a livenessProbe that can be used to restart
                            # deadlocked processes. You can find out more about it here:
                            # https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-liveness-command
                            #
                            # We don't use a livenessProbe as it's easy to cause unnecessary
                            # restarts, which can be really disruptive to a site's availability.
                            # If you think your application is likely to be unstable after running
                            # for long periods send a note to reviz@allenai.org so we can work
                            # with you to craft the right livenessProbe.
                            readinessProbe: {
                                httpGet: workerHealthCheck + {
                                    path: '/'
                                },
                                periodSeconds: 10,
                                failureThreshold: 3
                            },
                            # This tells Kubernetes what CPU and memory resources your API needs.
                            # We set these values low by default, as most applications receive
                            # bursts of activity and accordingly don't need dedicated resources
                            # at all times.
                            #
                            # Your application will be allowed to use more resources than what's
                            # specified below. But your application might be killed if it uses
                            # more than what's requested. If you know you need more memory
                            # or that your workload is CPU intensive, consider increasing the
                            # values below.
                            #
                            # For more information about these values, and the current maximums
                            # that your application can request, see:
                            # https://skiff.allenai.org/resources.html
                            resources: {
                                requests: {
                                    cpu: 63,
                                    memory: '257G'
                                },
                            },
                            env: envVariables + [
                                {
                                    name: 'OTEL_SERVICE_NAME',
                                    value: 'infinigram-api-worker'
                                },
                                {
                                    name: 'ASSIGNED_INDEX',
                                    value: indexId
                                }
                            ]
                        }
                    ]
                }
            }
        }
    };
    
    attributionWorkerDeployment

  